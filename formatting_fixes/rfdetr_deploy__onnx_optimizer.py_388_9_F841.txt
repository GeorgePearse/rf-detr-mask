def insert_layernorm_plugin(self):
        n_layer_norm_plugin = 0
        for node in self.graph.nodes:
            if (
                node.op == "ReduceMean"
                and node.o().op == "Sub"
                and node.o().inputs[0] == node.inputs[0]
                and node.o().o(0).op == "Pow"
                and node.o().o(1).op == "Div"
                and node.o().o(0).o().op == "ReduceMean"
                and node.o().o(0).o().o().op == "Add"
                and node.o().o(0).o().o().o().op == "Sqrt"
                and node.o().o(0).o().o().o().o().op == "Div"
                and node.o().o(0).o().o().o().o() == node.o().o(1)
                and node.o().o(0).o().o().o().o().o().op == "Mul"
                and node.o().o(0).o().o().o().o().o().o().op == "Add"
                and len(node.o().o(0).o().o().o().o().o().inputs[1].values.shape) == 1
            ):
                input_tensor = (
                    node.inputs[0] if node.i().op == "Add" else node.i().inputs[0]
                )  # CLIP or UNet and VAE

                gamma_node = node.o().o().o().o().o().o().o()
                index = [isinstance(i, gs.ir.tensor.Constant) for i in gamma_node.inputs].index(True)
                gamma = np.array(
                    deepcopy(gamma_node.inputs[index].values.tolist()), dtype=np.float32
                )
                constant_gamma = gs.Constant(
                    "LayerNormGamma-" + str(n_layer_norm_plugin),
                    np.ascontiguousarray(gamma.reshape(-1)),
                )  # MUST use np.ascontiguousarray, or TRT will regard the shape of this Constant as (0) !!!

                beta_node = gamma_node.o()
                index = [isinstance(i, gs.ir.tensor.Constant) for i in beta_node.inputs].index(True)
                beta = np.array(deepcopy(beta_node.inputs[index].values.tolist()), dtype=np.float32)
                constant_beta = gs.Constant(
                    "LayerNormBeta-" + str(n_layer_norm_plugin), np.ascontiguousarray(beta.reshape(-1))
                )

                input_list = [input_tensor, constant_gamma, constant_beta]
                layer_norm_v = gs.Variable(
                    "LayerNormV-" + str(n_layer_norm_plugin), np.dtype(np.float32), input_tensor.shape
                )
                layer_norm_n = gs.Node(
                    "LayerNorm",
                    "LayerNormN-" + str(n_layer_norm_plugin),
                    inputs=input_list,
                    attrs=OrderedDict([("epsilon", 1.0e-5)]),
                    outputs=[layer_norm_v],
                )
                self.graph.nodes.append(layer_norm_n)
                n_layer_norm_plugin += 1

                if beta_node.outputs[0] in self.graph.outputs:
                    index = self.graph.outputs.index(beta_node.outputs[0])
                    self.graph.outputs[index] = layer_norm_v
                else:
                    last_node = beta_node.o() if beta_node.o().op == "Cast" else beta_node
                    for sub_node in self.graph.nodes:
                        if last_node.outputs[0] in sub_node.inputs:
                            index = sub_node.inputs.index(last_node.outputs[0])
                            sub_node.inputs[index] = layer_norm_v
                    last_node.outputs = []
        
        self.cleanup()
        return n_layer_norm_plugin

