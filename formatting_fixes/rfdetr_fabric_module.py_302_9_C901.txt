```python
def validation_step(self, batch):
    """Execute a single validation step.

    Args:
        batch: Tuple of (samples, targets)

    Returns:
        tuple: (metrics, results, targets) for COCO evaluation
    """
    samples, targets = batch
    self.model.eval()
    self.criterion.eval()

    # Prepare model for evaluation
    model_to_eval, samples, orig_dtype = self._prepare_model_for_evaluation(samples)
    
    # Forward pass
    with torch.no_grad(), torch.autocast(**self.autocast_args):
        outputs = model_to_eval(samples)
    
    # Handle FP16 conversion if needed
    if orig_dtype is not None:
        outputs = self._convert_outputs_to_float(outputs, model_to_eval, orig_dtype)

    # Compute loss and metrics
    val_metrics = self._compute_validation_metrics(outputs, targets)
    
    # Process results for COCO evaluation
    orig_target_sizes = torch.stack([t["orig_size"] for t in targets], dim=0)
    results = self.postprocessors["bbox"](outputs, orig_target_sizes)
    res = {target["image_id"].item(): output for target, output in zip(targets, results)}
    
    return {"metrics": val_metrics, "results": res, "targets": targets}

def _prepare_model_for_evaluation(self, samples):
    """Prepare model for evaluation, handling EMA and FP16 settings."""
    # Determine which model to evaluate (EMA or regular)
    model_to_eval = self.ema.ema if self.ema is not None else self.model
    
    # Check if using FP16 for evaluation
    if isinstance(self.config, dict):
        fp16_eval = self.config.get("fp16_eval", False)
    else:
        fp16_eval = getattr(self.config, "fp16_eval", False)
    
    # Store original precision and convert if needed
    orig_dtype = None
    if fp16_eval:
        orig_dtype = next(model_to_eval.parameters()).dtype
        model_to_eval = model_to_eval.half()
        samples = samples.half()
    
    return model_to_eval, samples, orig_dtype

def _convert_outputs_to_float(self, outputs, model_to_eval, orig_dtype):
    """Convert model outputs from FP16 back to original precision."""
    # Convert model back to original precision
    for p in model_to_eval.parameters():
        p.data = p.data.to(orig_dtype)
    
    # Convert outputs back to float
    for key in outputs:
        if key == "enc_outputs":
            for sub_key in outputs[key]:
                outputs[key][sub_key] = outputs[key][sub_key].float()
        elif key == "aux_outputs":
            for idx in range(len(outputs[key])):
                for sub_key in outputs[key][idx]:
                    outputs[key][idx][sub_key] = outputs[key][idx][sub_key].float()
        else:
            outputs[key] = outputs[key].float()
    
    return outputs

def _compute_validation_metrics(self, outputs, targets):
    """Compute validation metrics from model outputs."""
    # Compute loss
    loss_dict = self.criterion(outputs, targets)
    weight_dict = self.criterion.weight_dict
    
    # Log reduced loss
    loss_dict_reduced = utils.reduce_dict(loss_dict)
    loss_dict_reduced_scaled = {
        k: v * weight_dict[k] for k, v in loss_dict_reduced.items() if k in weight_dict
    }
    loss_dict_reduced_unscaled = {f"{k}_unscaled": v for k, v in loss_dict_reduced.items()}
    losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())
    
    # Store metrics
    val_metrics = {
        "loss": losses_reduced_scaled.item(),
        "class_error": loss_dict_reduced["class_error"].item(),
        **{k: v.item() for k, v in loss_dict_reduced_scaled.items()},
        **{k: v.item() for k, v in loss_dict_reduced_unscaled.items()},
    }
    self.val_metrics.append(val_metrics)
    
    return val_metrics
```

