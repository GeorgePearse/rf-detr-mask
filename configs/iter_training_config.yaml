# Iteration-based training configuration for RF-DETR-Mask

# Basic training parameters
max_steps: 2000
batch_size: 2
grad_accum_steps: 4

# Training scheduling
val_frequency: 200
checkpoint_frequency: 500
early_stopping: true 
early_stopping_patience: 5
early_stopping_min_delta: 0.001

# Learning rate and optimization
lr: 5.0e-5
lr_encoder: 5.0e-6
lr_projector: 5.0e-6
lr_scheduler: "cosine"
warmup_ratio: 0.1
lr_min_factor: 1.0e-5
weight_decay: 1.0e-4

# Mixed precision
amp: true
fp16_eval: false

# Training regularization
clip_max_norm: 0.5
dropout: 0.0
drop_path: 0.1

# Data parameters
dataset_file: "coco"
coco_path: "/home/georgepearse/data/cmr/annotations"
coco_train: "2025-05-15_12:38:23.077836_train_ordered.json"
coco_val: "2025-05-15_12:38:38.270134_val_ordered.json"
coco_img_path: "/home/georgepearse/data/images"
num_workers: 4
resolution: 336
square_resize: true
multi_scale: false
expanded_scales: false

# Model parameters
num_classes: 69
encoder: "dinov2_windowed_small"
pretrain_weights: null
layer_norm: true

# Transformer parameters
hidden_dim: 256
dec_layers: 3
dim_feedforward: 1024
sa_nheads: 8
ca_nheads: 8
group_detr: 1
two_stage: true
lite_refpoint_refine: true
num_queries: 100
bbox_reparam: true

# Loss parameters
cls_loss_coef: 4.5
bbox_loss_coef: 2.0
giou_loss_coef: 1.0
mask_loss_coef: 1.0 
dice_loss_coef: 1.0
aux_loss: true

# Matcher parameters
set_cost_class: 5.0
set_cost_bbox: 2.0
set_cost_giou: 1.0

# EMA parameters
use_ema: true
ema_decay: 0.993

# Output parameters
output_dir: "output_iter_training"

# Logging parameters
tensorboard: true
wandb: false