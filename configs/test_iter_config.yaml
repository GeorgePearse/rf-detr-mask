# Test configuration for iteration-based training with a small dataset

# Basic training parameters
max_steps: 200
batch_size: 1
grad_accum_steps: 1

# Training scheduling - identical frequency for validation and checkpoints
val_frequency: 25
checkpoint_frequency: 25
early_stopping: false 

# Learning rate and optimization
lr: 5.0e-5
lr_encoder: 5.0e-6
lr_projector: 5.0e-6
lr_scheduler: "cosine"
warmup_ratio: 0.1
lr_min_factor: 1.0e-5
weight_decay: 1.0e-4
lr_vit_layer_decay: 0.7
lr_component_decay: 1.0

# Mixed precision
amp: true

# Data parameters
dataset_file: "coco"
coco_path: "/home/georgepearse/data/cmr/annotations"
coco_train: "2025-05-15_12:38:23.077836_train_ordered.json"
coco_val: "2025-05-15_12:38:38.270134_val_ordered.json"
coco_img_path: "/home/georgepearse/data/images"
num_workers: 2
resolution: 518  # Must match dinov2_small.json image_size
test_limit: 100  # Limit dataset size for faster testing

# Model parameters
num_classes: 69
encoder: "dinov2_small"  # Use non-windowed version
layer_norm: true

# Transformer parameters
hidden_dim: 128  # Reduced for testing
dec_layers: 2    # Reduced for testing
dim_feedforward: 512
num_queries: 100
sa_nheads: 8
ca_nheads: 8
group_detr: 1
two_stage: true
lite_refpoint_refine: true
bbox_reparam: true
dec_n_points: 4
decoder_norm: "LN"

# Feature extraction parameters
out_feature_indexes: [9, 10, 11]  # Which layers to extract from backbone
projector_scale: ["P3", "P4", "P5"]  # Feature pyramid levels
window_block_indexes: null
use_cls_token: false
position_embedding: "sine"

# Output parameters
output_dir: "output_test_iter_training"

# Logging parameters
tensorboard: true
wandb: false